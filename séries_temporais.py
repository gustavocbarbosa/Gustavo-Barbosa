# -*- coding: utf-8 -*-
"""Séries Temporais.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uGamrjndeHioZ34aAKKTpl80POfnCI8K
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import spacy
import nltk
import gensim
import gdown
import os
import re
import seaborn as sns

def download(id):
  url = 'https://drive.google.com/file/d/' + str(id)
  gdown.download(url, output = None, quiet = False)

download('170LU8Ct7TE2hCr6pxvpBWdOzSx5v2mgI')

df = pd.read_csv('daily-min-temperatures.csv')
df

df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format=True)
indexeddf = df.set_index(['Date'])

indexeddf

plt.figure(figsize=(30,6))
sns.lineplot(data=indexeddf)

"""Pelo gráfico é possível perceber que a distribuição de temperaturas é ciclica durante os anos(o que já era de se esperar)

*   Podemos considerar os dados como estacionários? Não pois a média e DP não são constantes e os dados são sazonais

"""

# Rolling Statistics - Médias Móveis
rolmean = indexeddf.rolling(window=12).mean()
rolstd = indexeddf.rolling(window=12).std()

rolmean.head(12)

rolstd.head(12)

plt.figure(figsize=(30,6))
orig = plt.plot(indexeddf, color='blue', label='Original')
mean = plt.plot(rolmean, color='red', label='Rolling Mean')
std = plt.plot(rolstd, color='black', label='Rolling Std')
plt.show(block=False)

from statsmodels.tsa.seasonal import seasonal_decompose
decompose  = seasonal_decompose(indexeddf.resample('M').sum(),freq=12)
fig = plt.figure()
fig = decompose.plot()
fig.set_size_inches(12,8)

"""# Teste de Dickey Fuller Aumentado

Referência: https://medium.com/@cmukesh8688/why-is-augmented-dickey-fuller-test-adf-test-so-important-in-time-series-analysis-6fc97c6be2f0#:~:text=Augmented%20Dickey%20Fuller%20test%20(ADF%20Test)%20is%20a,Stationary%20is%20very%20important%20factor%20on%20time%20series.
"""

from statsmodels.tsa.stattools import adfuller

print('Resultados do Teste de Dickey Fuller Aumentado')
dftest = adfuller(indexeddf['Temp'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Estatisticas de Teste', 'p-value', 'Número de Lags usadas', "Numero de observações usadas"])
for key,value in dftest[4].items():
  dfoutput['Critical Value (%s)'%key] = value

print(dfoutput)

"""Obtivemos um p-value baixo o que é interessante e valores críticos sempre maiores que as estatísticas do teste

# Análises Logaritmicas
"""

log_indexeddf = np.log(indexeddf)
plt.figure(figsize=(30,6))
plt.plot(log_indexeddf)

# Rolling Statistics - Médias Móveis
log_rolmean = log_indexeddf.rolling(window=12).mean()
log_rolstd = log_indexeddf.rolling(window=12).std()

plt.figure(figsize=(30,6))
orig = plt.plot(log_indexeddf, color='blue', label='Original')
mean = plt.plot(log_rolmean, color='red', label='Rolling Mean')
std = plt.plot(log_rolstd, color='black', label='Rolling Std')
plt.show(block=False)

logmean_df = log_indexeddf - log_rolmean
logmean_df.head(12)
logmean_df.dropna(inplace=True)
logmean_df.head(12)

logmean_rolmean = logmean_df.rolling(window=12).mean()
logmean_rolstd = logmean_df.rolling(window=12).std()

plt.figure(figsize=(30,6))
orig = plt.plot(logmean_df, color='blue', label='Original')
mean = plt.plot(logmean_rolmean, color='red', label='Rolling Mean')
std = plt.plot(logmean_rolstd, color='black', label='Rolling Std')
plt.show(block=False)

print('Resultados do Teste de Dickey Fuller Aumentado')
dftest = adfuller(logmean_df['Temp'], autolag='AIC')

dfoutput = pd.Series(dftest[0:4], index=['Estatisticas de Teste', 'p-value', 'Número de Lags usadas', "Numero de observações usadas"])
for key,value in dftest[4].items():
  dfoutput['Critical Value (%s)'%key] = value

print(dfoutput)

"""# LSTM"""

import numpy
import matplotlib.pyplot as plt
import pandas
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

numpy.random.seed(7)

#Normalização do DataSet
scaler = MinMaxScaler(feature_range=(0,1))
logmean_df = scaler.fit_transform(logmean_df)

logmean_df

# Dividir entre teste e treino
train_size = int(len(logmean_df) * 0.67)
test_size = len(logmean_df) - train_size
train, test = logmean_df[0:train_size,:], logmean_df[train_size:len(logmean_df),:]
print(len(train), len(test))

def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return numpy.array(dataX), numpy.array(dataY)

look_back = 1
trainX, trainY = create_dataset(train)
testX, testY = create_dataset(test)

"""A rede LSTM espera o input com o seguinte formato: [samples,time steps, features], por isso precisamos mudar o formato com a função reshape:"""

# reshape input to be [samples, time steps, features]
trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))

trainX.shape

trainY.shape

trainX

# create and fit the LSTM network
model = Sequential()
model.add(LSTM(4, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)

# make predictions
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)
# invert predictions
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])
# calculate root mean squared error
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))

plt.figure(figsize=(30,6))
# shift train predictions for plotting
trainPredictPlot = numpy.empty_like(logmean_df)
trainPredictPlot[:, :] = numpy.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(logmean_df)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(logmean_df)-1, :] = testPredict
# plot baseline and predictions
plt.plot(scaler.inverse_transform(logmean_df))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

"""Window Method"""

# convert an array of values into a dataset matrix
def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return numpy.array(dataX), numpy.array(dataY)
# fix random seed for reproducibility
numpy.random.seed(7)
# load the dataset
dataframe = pd.read_csv('daily-min-temperatures.csv', usecols=[1], engine='python')
dataset = dataframe.values
dataset = dataset.astype('float32')
# normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)
# split into train and test sets
train_size = int(len(dataset) * 0.67)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
# reshape into X=t and Y=t+1
look_back = 3
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)
# reshape input to be [samples, time steps, features]
trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
# create and fit the LSTM network
model = Sequential()
model.add(LSTM(4, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)
# make predictions
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)
# invert predictions
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])
# calculate root mean squared error
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# shift train predictions for plotting
trainPredictPlot = numpy.empty_like(dataset)
trainPredictPlot[:, :] = numpy.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = numpy.empty_like(dataset)
testPredictPlot[:, :] = numpy.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
# plot baseline and predictions

plt.figure(figsize=(30,6))
plt.plot(scaler.inverse_transform(dataset))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()