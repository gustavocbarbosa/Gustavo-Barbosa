# -*- coding: utf-8 -*-
"""Natural Language Processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13UT_3u4y4usoas0YyDCTO_acGb0nxczE

# Importações e obtenção dos dados
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import spacy
import nltk
import gensim
import gdown
import os
import re
import seaborn as sns

def download(id):
  url = 'https://drive.google.com/uc?id=' + str(id)
  gdown.download(url, output = None, quiet = False)

download('1ibC2jeMpZUImGtYSxLD--mM6TLGDycE7')

df = pd.read_csv('Musical_instruments_reviews.csv')
df

"""Only need columns reviewText and overall"""

df = df[['reviewText', 'overall']]
bin_df = df.copy()
df

bin_df['overall']=np.where(bin_df['overall']>=3,1,0)

bin_df

"""#Análise do Dataset"""

sns.countplot(x=df['overall'], data = df)

sns.countplot(x=bin_df['overall'], data = bin_df)

"""# Pré-processamento dos textos"""

import string
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize

from nltk.stem import PorterStemmer, WordNetLemmatizer
ps = PorterStemmer()
lemma = WordNetLemmatizer()
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
punc = string.punctuation
stop_words = set(stopwords.words('english') + list(punc))

def preprocess(x, how = 'stem'):
  text = x.lower()
  text = text.replace('\r', ' ').replace('\n', ' ').replace('\t', ' ')
  tok = word_tokenize(text)
  if how == 'stem':
    out = ' '.join([ps.stem(w) for w in tok if not w in stop_words])
  elif how == 'lemma':
    out = ' '.join([lemma.lemmatize(w) for w in tok if not w in stop_words])
  return out

df['Stemming'] = df['reviewText'].astype(str).apply(lambda x: preprocess(x, how = 'stem'))
df['Lemmatization'] = df['reviewText'].astype(str).apply(lambda x: preprocess(x, how = 'lemma'))

for i in range(3):
  print(f'Original:\n{df["reviewText"][i]}\nStemming:\n{df["Stemming"][i]}\nLemmatization:\n{df["Lemmatization"][i]}\n\n')

"""# Bag-of-words, TFIDF e ML Clássico"""

from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import classification_report

def create_X_y(data, feats, labels = 'overall', **kwargs):
  X = data[feats]
  # y = to_categorical(data[labels])
  y = data[labels]
  X_train, X_test, y_train, y_test = train_test_split(X, y, **kwargs)
  return X_train, X_test, y_train, y_test

X_train, X_test, y_train, y_test = create_X_y(data = df, feats = 'Stemming', test_size = 0.25, random_state = 42)

bow = CountVectorizer()
vec = bow.fit_transform(X_train)
vec_test = bow.transform(X_test)

clf = MultinomialNB()
clf.fit(vec, y_train)
pred = clf.predict(vec_test)
print(classification_report(y_test, pred))

clf = LogisticRegression(max_iter=10000)
clf.fit(vec, y_train)
pred = clf.predict(vec_test)
print(classification_report(y_test, pred))

clf = SVC()
clf.fit(vec, y_train)
pred = clf.predict(vec_test)
print(classification_report(y_test, pred))

X_train, X_test, y_train, y_test = create_X_y(data = df, feats = 'Lemmatization', test_size = 0.25, random_state = 42)

vec = bow.fit_transform(X_train)
vec_test = bow.transform(X_test)
clf = MultinomialNB()
clf.fit(vec, y_train)
pred = clf.predict(vec_test)
print(classification_report(y_test, pred))

clf = LogisticRegression(max_iter=10000)
clf.fit(vec, y_train)
pred = clf.predict(vec_test)
print(classification_report(y_test, pred))

clf = SVC()
clf.fit(vec, y_train)
pred = clf.predict(vec_test)
print(classification_report(y_test, pred))

"""# Tokenização com Tensorflow"""

X_train, X_test, y_train, y_test = create_X_y(data = df, feats = 'Lemmatization', test_size = 0.25, random_state = 42)

from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

seq_train = tokenizer.texts_to_sequences(X_train)
seq_test = tokenizer.texts_to_sequences(X_test)

from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

maxlen = 60
X_train_tf = pad_sequences(seq_train, padding='post', maxlen=maxlen)
X_test_tf = pad_sequences(seq_test, padding='post', maxlen=maxlen)
y_train_tf = to_categorical(y_train)
y_test_tf = to_categorical(y_test)

"""# RNN básica do zero"""

from keras.models import Sequential
from keras.layers import Embedding, Dense, LSTM, SimpleRNN, Bidirectional
from keras.optimizers.schedules import ExponentialDecay
from keras.callbacks import EarlyStopping, LearningRateScheduler
from keras.optimizers import Adam
from keras.regularizers import l2

vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 200
n_classes = y_test_tf.shape[1]

rnn = Sequential([Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),
                    SimpleRNN(128, dropout=0.3),
                    Dense(n_classes, activation = 'softmax')])

lr_schedule = ExponentialDecay(
                              0.01,
                              decay_steps=500,
                              decay_rate=0.96,
                               )

rnn.compile(optimizer = Adam(learning_rate = lr_schedule),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

rnn.summary()

early_stop = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)

rnn_history = rnn.fit(x = X_train_tf, 
                        y = y_train_tf, 
                        batch_size=32, 
                        epochs = 30, 
                        verbose = 1, 
                        validation_data= (X_test_tf, y_test_tf),
                        callbacks = [early_stop]
                        )

def accuracy_history(history):
  acc = history.history['accuracy' ]
  val_acc = history.history['val_accuracy' ]
  loss = history.history['loss' ]
  val_loss = history.history['val_loss' ]

  epochs = range(len(acc))

  plt.plot(epochs, acc, label = 'Training')
  plt.plot(epochs, val_acc, label = 'Validation')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.show()

  plt.plot(epochs, loss, label = 'Training')
  plt.plot(epochs, val_loss, label = 'Validation')
  plt.legend()
  plt.title('Training and validation loss')
  plt.show()

accuracy_history(rnn_history)

"""# LSTM unidirecional"""

embedding_dim = 100
lstm = Sequential([Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),
                    LSTM(32, dropout=0.2, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)),
                    Dense(n_classes, activation = 'softmax')])

lstm.compile(optimizer = Adam(learning_rate = lr_schedule),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

lstm.summary()

lstm_history = lstm.fit(x = X_train_tf, 
                        y = y_train_tf, 
                        batch_size=32, 
                        epochs = 30, 
                        verbose = 1, 
                        validation_data= (X_test_tf, y_test_tf),
                        callbacks = [early_stop]
                        )

accuracy_history(lstm_history)

"""# LSTM bidirecional"""

embedding_dim = 50
bi_lstm = Sequential([Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),
                    Bidirectional(LSTM(32, dropout=0.4, kernel_regularizer=l2(0.1), 
                                       recurrent_regularizer=l2(0.1), bias_regularizer=l2(0.1))),
                    Dense(n_classes, activation = 'softmax')])

bi_lstm.compile(optimizer = Adam(learning_rate = lr_schedule),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

bi_lstm.summary()

bilstm_history = bi_lstm.fit(x = X_train_tf, 
                        y = y_train_tf, 
                        batch_size=32, 
                        epochs = 30, 
                        verbose = 1, 
                        validation_data= (X_test_tf, y_test_tf),
                        callbacks = [early_stop]
                        )

accuracy_history(bilstm_history)

"""
# Word Embeddings pré-treinadas
"""

word_index = tokenizer.word_index

def glove_embeddings(file, embedding_dimension = 100, vocab = word_index):
  embeddings_index = {}
  with open(file) as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
  embedding_matrix = np.zeros((len(vocab)+1, embedding_dimension))
  for word, i in vocab.items():
      embedding_vector = embeddings_index.get(word)
      if embedding_vector is not None:
          embedding_matrix[i] = embedding_vector
  return embedding_matrix

def word2vec_embeddings(filename, embedding_dimension = 300, vocab = word_index, startline = 4, pos_present = True):
    file = open(filename,'r')
    lines = file.readlines()[startline:]
    file.close()
    embedding = dict()
    for line in lines:
      parts = line.split()
      if pos_present:
        word = re.match('(.*)_.*', parts[0]).group(1)
      else:
        word = parts[0]
      embedding[word] = np.asarray(parts[1:], dtype='float32')
    vocab_size = len(vocab) + 1
    weight_matrix = np.zeros((vocab_size, embedding_dimension))
    for word, i in vocab.items():
      embedding_vector = embedding.get(word)
      if embedding_vector is not None:
          weight_matrix[i] = embedding_vector
    return embedding, weight_matrix

"""## GloVe"""

# Commented out IPython magic to ensure Python compatibility.
!mkdir GloVe/
# %cd GloVe/
download('1fEmZiEebta2rkAwsG4rmmsn8A8uk7xST')
!unzip glove.6B.zip
!rm glove.6B.zip
# %cd ..

glove_weights = glove_embeddings('GloVe/glove.6B.100d.txt')

embedding_dim = 100

cb = EarlyStopping(monitor = 'val_accuracy', patience = 5, restore_best_weights = True)

glove_lstm = Sequential([
                          Embedding(input_dim = vocab_size, output_dim = embedding_dim, 
                                    weights = [glove_weights], input_length = maxlen, 
                                    trainable = False
                                    ),
                          Bidirectional(LSTM(256, dropout=0.2, kernel_regularizer=l2(0.01), 
                                            recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01))),
                          Dense(n_classes, activation = 'softmax')
                        ])

glove_lstm.compile(optimizer = Adam(learning_rate = lr_schedule),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

glove_lstm.summary()

glove_lstm_history = glove_lstm.fit(x = X_train_tf, 
                        y = y_train_tf, 
                        batch_size=32, 
                        epochs = 30, 
                        verbose = 1, 
                        validation_data= (X_test_tf, y_test_tf),
                        callbacks = [cb]
                        )

accuracy_history(glove_lstm_history)

"""## Word2Vec"""

# Commented out IPython magic to ensure Python compatibility.
!mkdir Word2Vec/
# %cd Word2Vec/
download('1BFaWuZT9L2pFte0lMGltAaikMyx9aKbL')
!unzip word2vec_skipgram_300d_250k.zip
!rm word2vec_skipgram_300d_250k.zip
# %cd ..

embedding_dict, word2vec_weights = word2vec_embeddings('Word2Vec/model.txt')

embedding_dim = 300

cb = EarlyStopping(monitor = 'val_accuracy', patience = 3, restore_best_weights = True)

word2vec_lstm = Sequential([
                          Embedding(input_dim = vocab_size, output_dim = embedding_dim, 
                                    weights = [word2vec_weights], input_length = maxlen, 
                                    trainable = False
                                    ),
                          Bidirectional(LSTM(256, dropout=0.2, kernel_regularizer=l2(0.01), 
                                            recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01))),
                          Dense(n_classes, activation = 'softmax')
                        ])

word2vec_lstm.compile(optimizer = Adam(learning_rate = lr_schedule),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

word2vec_lstm.summary()

word2vec_lstm_history = word2vec_lstm.fit(x = X_train_tf, 
                        y = y_train_tf, 
                        batch_size=32, 
                        epochs = 30, 
                        verbose = 1, 
                        validation_data= (X_test_tf, y_test_tf),
                        callbacks = [cb]
                        )

accuracy_history(word2vec_lstm_history)

"""## FastText

O Colab aparentemente não tem RAM o suficiente pra carregar o fastText
"""

# !mkdir fastText/
# %cd fastText/
# download('1t2KaBTY5Acgfg3SiFibQwMfSL6oOVNpZ')
# !unzip fasttextwiki-news-300d-1M.vec.zip
# !rm fasttextwiki-news-300d-1M.vec.zip
# %cd ..

# import io

# def load_vectors(fname):
#     fin = io.open(fname, 'r', encoding='utf-8', newline='\n', errors='ignore')
#     n, d = map(int, fin.readline().split())
#     data = {}
#     for line in fin:
#         tokens = line.rstrip().split(' ')
#         data[tokens[0]] = map(float, tokens[1:])
#     return data

# fastText_vec = load_vectors('fastText/wiki-news-300d-1M.vec')

"""# Topic Modelling

## Extração
"""

from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from pprint import pprint
from sklearn.decomposition import NMF, LatentDirichletAllocation

tfidf = TfidfVectorizer()
X = tfidf.fit_transform(df['Lemmatization'].values)
featnames = tfidf.get_feature_names()

# Run LDA
lda3 = LatentDirichletAllocation(n_components = 3, learning_method='online', random_state = 0)
lda5 = LatentDirichletAllocation(n_components = 5, learning_method='online', random_state = 0)

lda3.fit(X)
lda5.fit(X)

lda = {3: lda3, 5: lda5}

"""## Visualização"""

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print ("Topic %d:" % (topic_idx))
        print (" ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))

no_top_words = 10
for i in lda:
    print(f'LDA {i} topics:\n')
    display_topics(lda[i], featnames, no_top_words)
    print('\n' + '='*40 + '\n')

"""# Resumo das acurácias"""

results = {'RNN básica': {'Treino': 0.68, 'Teste': 0.65, 'Épocas': 3}, 'LSTM unidirecional': {'Treino': 0.65, 'Teste': 0.67, 'Épocas': 2},
           'LSTM bidirecional': {'Treino': 0.66, 'Teste': 0.67, 'Épocas': 1}, 'GloVe': {'Treino': 0.65, 'Teste': 0.67, 'Épocas': 1},
           'Word2Vec': {'Treino': 0.65, 'Teste': 0.67, 'Épocas': 1}}
pd.DataFrame(results).transpose().astype({'Épocas': int})